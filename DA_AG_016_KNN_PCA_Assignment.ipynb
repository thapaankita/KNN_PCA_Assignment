{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#KNN & PCA Assignment"
      ],
      "metadata": {
        "id": "CYXbrWyGX0c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**K-Nearest Neighbors (KNN):**  \n",
        "KNN is a **supervised machine learning algorithm** used for both **classification** and **regression** tasks.  \n",
        "It is a **non-parametric** and **instance-based (lazy learning)** method, meaning it does not assume any specific distribution about the data and does not explicitly build a model during training. Instead, it memorizes the training data and makes predictions at query time.\n",
        "\n",
        "### How KNN Works:\n",
        "1. Choose a value of **K** (number of nearest neighbors).\n",
        "2. For a new data point:\n",
        "   - Calculate the **distance** between the new point and all training data points.  \n",
        "     (Common metrics: Euclidean, Manhattan, Minkowski, etc.)\n",
        "   - Identify the **K closest points** (neighbors).\n",
        "3. Make predictions based on these neighbors.\n",
        "\n",
        "### KNN for Classification:\n",
        "- Each of the K neighbors \"votes\" for its class.\n",
        "- The class with the majority vote is assigned to the new data point.  \n",
        "\n",
        "**Example:** If K=5 and among neighbors: 3 belong to class A and 2 belong to class B →  \n",
        "the new point is classified as **class A**.\n",
        "\n",
        "### KNN for Regression:\n",
        "- Instead of voting, KNN takes the **average (or weighted average)** of the target values of the K nearest neighbors.\n",
        "- The predicted value is a continuous number.  \n",
        "\n",
        "**Example:** If K=3 and neighbors have target values 10, 12, 14 →  \n",
        "predicted value = **(10 + 12 + 14) / 3 = 12**.\n",
        "\n",
        "### Key Points:\n",
        "- KNN is simple and effective for smaller datasets.\n",
        "- The choice of **K** is important:  \n",
        "  - Small K → sensitive to noise (**overfitting**)  \n",
        "  - Large K → smoother decision boundary but may **underfit**\n",
        "- Requires **feature scaling** (normalization/standardization) since distance measures are used.\n",
        "\n",
        "## Q2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**Curse of Dimensionality:**  \n",
        "The Curse of Dimensionality refers to the various problems that arise when working with data that has a very high number of features (dimensions). As the number of dimensions increases:\n",
        "\n",
        "- The **volume of the feature space increases exponentially**, making the data points sparse.\n",
        "- The **distance between points becomes less meaningful**, because all points tend to appear similarly far from each other.\n",
        "- This sparsity makes it difficult for algorithms like KNN to find truly \"nearest\" neighbors.\n",
        "\n",
        "### How it affects KNN performance:\n",
        "1. **Distance measure loses effectiveness:**  \n",
        "   KNN relies on distance metrics (like Euclidean distance) to find neighbors. In high dimensions, the difference in distance between the nearest and farthest neighbors becomes very small, making neighbor selection unreliable.\n",
        "\n",
        "2. **Increased computation:**  \n",
        "   More features mean more calculations for each distance computation, slowing down prediction.\n",
        "\n",
        "3. **Overfitting risk:**  \n",
        "   With many features, KNN may fit noise instead of true patterns, reducing generalization performance.\n",
        "\n",
        "### Mitigation:\n",
        "- **Dimensionality reduction techniques** like PCA (Principal Component Analysis) are commonly used to reduce the number of features while retaining most of the variance.\n",
        "- Feature selection or normalization can also help improve KNN performance in high-dimensional spaces.\n",
        "\n",
        "## Q3. What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**Principal Component Analysis (PCA):**  \n",
        "PCA is an **unsupervised dimensionality reduction technique** used to reduce the number of features in a dataset while retaining most of the original variance. It transforms the original features into a new set of **uncorrelated variables** called **principal components**, which are linear combinations of the original features.\n",
        "\n",
        "- The first principal component captures the **maximum variance** in the data.\n",
        "- The second principal component captures the maximum remaining variance, and so on.\n",
        "- Typically, only the top few principal components are retained to reduce dimensionality.\n",
        "\n",
        "**Difference from Feature Selection:**  \n",
        "\n",
        "| Aspect                     | PCA (Feature Extraction)               | Feature Selection                     |\n",
        "|-----------------------------|---------------------------------------|--------------------------------------|\n",
        "| Approach                    | Creates **new features** (principal components) | Selects a **subset of original features** |\n",
        "| Data Transformation         | Yes (linear combinations of original features) | No (keeps original features)        |\n",
        "| Goal                        | Reduce dimensionality while retaining variance | Keep most relevant features          |\n",
        "| Use Case                    | High-dimensional datasets where feature correlation exists | When some features are irrelevant or redundant |\n",
        "\n",
        "**Summary:**  \n",
        "- PCA reduces dimensions by **combining features** into principal components.  \n",
        "- Feature selection reduces dimensions by **choosing the most important features** without creating new ones.  \n",
        "- Both help in improving model performance, reducing overfitting, and speeding up computation, but they do it in fundamentally.\n",
        "\n",
        "## Q4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**Eigenvectors and Eigenvalues in PCA:**  \n",
        "PCA uses **linear algebra** to transform the original features into principal components. This involves computing the **covariance matrix** of the data and then finding its **eigenvectors** and **eigenvalues**.\n",
        "\n",
        "- **Eigenvectors:**  \n",
        "  These are vectors that define the **directions** of the new feature space (principal components). Each eigenvector points in a direction along which the data varies the most.\n",
        "\n",
        "- **Eigenvalues:**  \n",
        "  These are scalars that measure the **magnitude of variance** along each eigenvector. A larger eigenvalue indicates that the corresponding eigenvector captures more of the data's variance.\n",
        "\n",
        "**Importance in PCA:**\n",
        "1. **Determine principal components:**  \n",
        "   Eigenvectors with the largest eigenvalues are selected as principal components because they capture the most significant variation in the data.\n",
        "\n",
        "2. **Dimensionality reduction:**  \n",
        "   By keeping only the top eigenvectors (with highest eigenvalues), we reduce the number of dimensions while preserving most of the information.\n",
        "\n",
        "3. **Feature transformation:**  \n",
        "   Eigenvectors provide the new axes for the transformed feature space, and eigenvalues quantify how much variance each axis explains.\n",
        "\n",
        "**Summary:**  \n",
        "- Eigenvectors → Directions of principal components  \n",
        "- Eigenvalues → Importance (variance) of each principal component  \n",
        "- PCA chooses eigenvectors with largest eigenvalues to reduce dimensions efficiently.\n",
        "\n",
        "## Q5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**Combining PCA and KNN:**  \n",
        "- **KNN** is sensitive to the **curse of dimensionality**: as the number of features increases, distances between points become less meaningful, which can degrade KNN performance.  \n",
        "- **PCA** reduces the number of features by creating **principal components** that capture most of the variance, while eliminating redundant or less informative features.\n",
        "\n",
        "**How they complement each other:**\n",
        "1. **Dimensionality Reduction:**  \n",
        "   PCA reduces high-dimensional data to a smaller set of components, making distance calculations in KNN more meaningful.\n",
        "\n",
        "2. **Improved KNN Performance:**  \n",
        "   With fewer dimensions, KNN can classify or regress more accurately, and it becomes less sensitive to noise.\n",
        "\n",
        "3. **Faster Computation:**  \n",
        "   Fewer features mean fewer distance calculations for KNN, which speeds up prediction.\n",
        "\n",
        "4. **Better Generalization:**  \n",
        "   By reducing irrelevant or correlated features, PCA helps KNN avoid overfitting and improves model generalization.\n",
        "\n",
        "## Q6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "We will train a KNN classifier on the Wine dataset twice:  \n",
        "1. **Without feature scaling**  \n",
        "2. **With feature scaling** (StandardScaler)  \n",
        "\n",
        "We will then compare the model accuracy in both cases.   \n"
      ],
      "metadata": {
        "id": "cY_5iTEqYTZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------------------------\n",
        "# 1. KNN without feature scaling\n",
        "# ---------------------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. KNN with feature scaling\n",
        "# ---------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQUHiyVFaPQo",
        "outputId": "8a5c23d5-cba1-49e8-d01b-9d570b9841e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "We will apply PCA to the Wine dataset to reduce dimensionality and understand how much variance each principal component captures.\n"
      ],
      "metadata": {
        "id": "A4Iio56JaQfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features before applying PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio for each principal component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, ratio in enumerate(explained_variance, start=1):\n",
        "    print(f\"Principal Component {i}: {ratio:.4f}\")\n",
        "\n",
        "# Optional: cumulative explained variance\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "print(\"\\nCumulative Explained Variance:\", cumulative_variance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4oOHIKPaV1F",
        "outputId": "3dd62bbd-dee1-466b-b597-1582bb8329fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n",
            "\n",
            "Cumulative Explained Variance: [0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
            " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "We will apply PCA to reduce the Wine dataset to the top 2 principal components and then train a KNN classifier.  \n",
        "We will compare the accuracy with the KNN trained on the original (scaled) dataset.\n"
      ],
      "metadata": {
        "id": "uzojMDCBaZBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ---------------------------\n",
        "# 1. KNN on original scaled data\n",
        "# ---------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train, y_train)\n",
        "y_pred_original = knn_original.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(\"Accuracy on original dataset:\", accuracy_original)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Apply PCA (top 2 components)\n",
        "# ---------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. KNN on PCA-transformed data\n",
        "# ---------------------------\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train_pca)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
        "print(\"Accuracy on PCA-transformed dataset (top 2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_K1gbuEae9B",
        "outputId": "167b51b9-c98e-42af-b464-a668f000e10e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9444444444444444\n",
            "Accuracy on PCA-transformed dataset (top 2 components): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "We will train KNN classifiers on the scaled Wine dataset using **Euclidean** and **Manhattan** distance metrics and compare their accuracies.\n"
      ],
      "metadata": {
        "id": "txYayX_xaiMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------------------------\n",
        "# 1. KNN with Euclidean distance\n",
        "# ---------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. KNN with Manhattan distance\n",
        "# ---------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEfQK3sJamn7",
        "outputId": "c6b10fa0-1c54-44ee-a1b8-0783fd5c09ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444444444444444\n",
            "Accuracy with Manhattan distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Explain how you would:  \n",
        "- Use PCA to reduce dimensionality  \n",
        "- Decide how many components to keep  \n",
        "- Use KNN for classification post-dimensionality reduction  \n",
        "- Evaluate the model  \n",
        "- Justify this pipeline to your stakeholders\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**1. Use PCA to reduce dimensionality:**  \n",
        "- Gene expression datasets often have **thousands of features (genes)** but relatively few samples.  \n",
        "- Directly applying KNN or other models can lead to **overfitting**.  \n",
        "- Apply **PCA** to transform the original features into **principal components**, which capture the most variance in the data.  \n",
        "- This reduces the dimensionality while retaining the most important information for classification.\n",
        "\n",
        "**2. Decide how many components to keep:**  \n",
        "- Examine the **explained variance ratio** of principal components.  \n",
        "- Retain enough components to capture **~90–95% of the total variance**.  \n",
        "- This balances **information retention** and **overfitting risk**.\n",
        "\n",
        "**3. Use KNN for classification post-dimensionality reduction:**  \n",
        "- Standardize the PCA-transformed features.  \n",
        "- Train a **KNN classifier** on the reduced feature set.  \n",
        "- KNN works well here because distances in lower-dimensional space are more meaningful and less noisy.\n",
        "\n",
        "**4. Evaluate the model:**  \n",
        "- Use **cross-validation** (e.g., k-fold) to assess model stability and generalization.  \n",
        "- Evaluate performance metrics such as **accuracy, precision, recall, F1-score**, or **ROC-AUC** depending on class imbalance.  \n",
        "- Compare results with and without PCA to demonstrate dimensionality reduction benefits.\n",
        "\n",
        "**5. Justify this pipeline to stakeholders:**  \n",
        "- **Robustness:** PCA reduces overfitting by removing redundant or noisy features.  \n",
        "- **Interpretability:** PCA highlights the most informative patterns in gene expression.  \n",
        "- **Efficiency:** Reduced feature space leads to faster model training and prediction.  \n",
        "- **Accuracy:** KNN on PCA-transformed data maintains strong classification performance while minimizing complexity.  \n",
        "- This approach is widely accepted in **biomedical data analysis**, where high-dimensional datasets are common and interpretability is important.\n"
      ],
      "metadata": {
        "id": "-WeC5kUsaqfb"
      }
    }
  ]
}